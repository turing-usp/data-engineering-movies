{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudo pré-Engenharia de Dados\n",
    "\n",
    "O objetivo desse notebook é fazer algumas verificações em relação ao consumo de memória e tempo de processamento dos jobs de Engenharia de Dados, bem como testar alguns snippets de código e comparar o desempenho de algumas operações de busca/inserção.\n",
    "\n",
    "Começamos importando as bibliotecas necessárias e configurando o ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ETL.settings import API_KEY, BASE_URL\n",
    "import src.ETL.tmdb_api as api\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import requests\n",
    "import json\n",
    "import locale\n",
    "from google.oauth2 import service_account\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "try:\n",
    "    locale.setlocale(locale.LC_ALL, 'pt_BR')\n",
    "except:\n",
    "    locale.setlocale(locale.LC_ALL, 'pt_BR.utf8')\n",
    "\n",
    "# Credenciais BigQuery\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    filename='/home/rodrigo/repos/data-engineering-movies/airflow-docker/dags/src/ETL/credentials.json',\n",
    "    scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "\n",
    "endpoint = '/discover/movie'\n",
    "\n",
    "# Filtros utilizados na busca\n",
    "params = {\n",
    "    'primary_release_date.gte': '2023-01-01',\n",
    "    'primary_release_date.lte': '2023-06-01',\n",
    "}\n",
    "\n",
    "def generate_date_intervals(initial_date: str, end_date: str, delta_time: int):\n",
    "    \"\"\"\n",
    "    Generates a list of date intervals between `initial_date` and `end_date` with `delta_time` increments.\n",
    "\n",
    "    initial_date: str\n",
    "        'YYYY-mm-dd'\n",
    "    \n",
    "    end_date: str\n",
    "        'YYYY-mm-dd'\n",
    "\n",
    "    delta_time: int\n",
    "        in months\n",
    "    \"\"\"\n",
    "\n",
    "    intervals = []\n",
    "\n",
    "    date_format =  '%Y-%m-%d'\n",
    "    initial_datetime = datetime.strptime(initial_date, date_format)\n",
    "    end_datetime = datetime.strptime(end_date, date_format)\n",
    "    \n",
    "    current_date = initial_datetime\n",
    "    while current_date < end_datetime:\n",
    "        intervals.append((current_date.strftime(date_format), (current_date + relativedelta(months=delta_time)).strftime(date_format)))\n",
    "        current_date += relativedelta(months=delta_time)\n",
    "    return intervals\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso objetivo é consultar o endpoint _**/discover/movie**_ da API do TMDB. Esse endpoint nos permite fazer queries na base de dados, paginando os resultados. Cada página possui informações sobre 20 filmes, e, para uma dada query, podemos pegar qualquer página entre 1 e 1000 de forma independente. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Façamos então uma primeira query ingênua, sem filtro algum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_discover_request(page=1, params={}):\n",
    "    params['page'] = page\n",
    "\n",
    "    default_params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        'language': 'pt-br'\n",
    "    }\n",
    "    \n",
    "    default_params.update(params)\n",
    "    return json.loads(\n",
    "        requests.get(BASE_URL + endpoint, params=default_params).content.decode()\n",
    "    )\n",
    "\n",
    "response = make_discover_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = response['total_pages']\n",
    "print(f'Há um total de {pages} páginas para a busca ingênua, o que dá aproximadamente {20 * pages} filmes')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "São MUITAS páginas de resultado. Isso não nos é útil, dado que a API nos permite somente pegar os resultados que estejam no intervalo de páginas de 1 a 1000."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mudemos então nossa estratégia. Utilizaremos o filtro de data, quebrando a base de dados em vários conjuntos disjuntos, e faremos a integração desses conjuntos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo: Primeiro integrar todos os filmes que foram lançados em 2023; depois, os de 2022; depois, 2021, e por aí vai. Será que isso funciona? Façamos então uma nova query para analisar se essa solução é viável.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'primary_release_date.gte': '2013-01-01',\n",
    "    'primary_release_date.lte': '2023-01-01',\n",
    "}\n",
    "\n",
    "make_discover_request(params=params)['total_pages']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para um período de 10 anos, temos mais que 10 mil páginas. Pelo Princípio das Casas de Pombo, segue que pelo menos um ano possui mais de mil páginas de filmes. Isso não nos é interessante, dado que podemos pegar as informações até a milésima página."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Façamos uma visualização da quantidade de filmes por semestre para termos uma melhor ideia da situação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_intervals = generate_date_intervals('1930-01-01', '2030-01-01', 120)\n",
    "\n",
    "decades = list()\n",
    "qt_movies = list()\n",
    "\n",
    "for date_interval in date_intervals:\n",
    "    response = make_discover_request(params={\n",
    "        'primary_release_date.gte': date_interval[0],\n",
    "        'primary_release_date.lte': date_interval[1],\n",
    "    })\n",
    "\n",
    "    decades.append(date_interval[0][:4])\n",
    "    qt_movies.append(response['total_results'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeira Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.barplot(x=decades, y=qt_movies)\n",
    "sns.despine(top=True, left=True)\n",
    "\n",
    "for i, v in enumerate(qt_movies):\n",
    "    plt.annotate(f\"{v:_.0f}\".replace('_', '.'), xy=(i, v), ha='center', va='bottom')\n",
    "\n",
    "plt.title('Quantidade de Filmes por Década no TMDB')\n",
    "\n",
    "# Retira graduação do eixo Y\n",
    "plot.yaxis.set_ticks([])\n",
    "\n",
    "# Aumenta largura\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 4)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste de ingestão"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos pegar dados sobre filmes do primeiro semestre de 2018 como um exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'primary_release_date.gte': '2018-01-01',\n",
    "    'primary_release_date.lte': '2018-07-01',\n",
    "}\n",
    "\n",
    "response = make_discover_request(params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['total_pages'], response['total_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pega_resultados_de_intervalo_de_paginas(inicio: int, fim: int, params: dict) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for page in range(inicio, fim):\n",
    "        response = make_discover_request(page, params=params)\n",
    "        if 'results' not in response:\n",
    "            print('Deu ruim, não tem resultados')\n",
    "            print(page)\n",
    "            print(response)\n",
    "            continue\n",
    "\n",
    "        results.extend(response['results'])\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def gera_intervalos(inicio, fim, qt_intervalos):\n",
    "    tamanho_intervalo = ceil((fim - inicio) / qt_intervalos)\n",
    "    intervalos = [(i, i + tamanho_intervalo) for i in range(inicio, fim, tamanho_intervalo)]\n",
    "\n",
    "    if intervalos[-1][1] > fim:\n",
    "        x = intervalos.pop()\n",
    "        intervalos.append((x[0], fim))\n",
    "    return intervalos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de páginas: 274\n",
      "Pegaremos os dados nos intervalos [(1, 70), (70, 139), (139, 208), (208, 275)]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'primary_release_date.gte': '2018-01-01',\n",
    "    'primary_release_date.lte': '2018-03-01',\n",
    "}\n",
    "\n",
    "response = make_discover_request(params = params)\n",
    "\n",
    "print('Total de páginas:', response['total_pages'])\n",
    "intervalos_pags = gera_intervalos(1, response['total_pages'] +1, 4)\n",
    "\n",
    "print('Pegaremos os dados nos intervalos', intervalos_pags)\n",
    "# Paraleliza processamento\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = [executor.submit(pega_resultados_de_intervalo_de_paginas, intervalo[0], intervalo[1], params) for intervalo in intervalos_pags]\n",
    "    resultados = []\n",
    "    for future in as_completed(futures):\n",
    "        # Obtém o resultado do objeto Future\n",
    "        df_resultado = future.result()\n",
    "\n",
    "        # Adiciona o resultado à lista de resultados\n",
    "        resultados.append(df_resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat(resultados, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5468\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5468 entries, 0 to 5467\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   adult              5468 non-null   bool   \n",
      " 1   backdrop_path      1851 non-null   object \n",
      " 2   genre_ids          5468 non-null   object \n",
      " 3   id                 5468 non-null   int64  \n",
      " 4   original_language  5468 non-null   object \n",
      " 5   original_title     5468 non-null   object \n",
      " 6   overview           5468 non-null   object \n",
      " 7   popularity         5468 non-null   float64\n",
      " 8   poster_path        4569 non-null   object \n",
      " 9   release_date       5468 non-null   object \n",
      " 10  title              5468 non-null   object \n",
      " 11  video              5468 non-null   bool   \n",
      " 12  vote_average       5468 non-null   float64\n",
      " 13  vote_count         5468 non-null   int64  \n",
      "dtypes: bool(2), float64(2), int64(2), object(8)\n",
      "memory usage: 523.4+ KB\n"
     ]
    }
   ],
   "source": [
    "print(response['total_results'])\n",
    "df_final.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que o dataframe resultante possui todos os dados referentes aos filmes no intervalo definido e consome somente 524 KB de memória. Estamos lidando portanto com um baixo volume de dados. O maior problema é em relação à API: traz pouquíssimos dados por vez (20 entradas por requisição) e demora muito (cerca de 600 ms por requisição)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A paralelização deve ajudar, diminuindo o tempo total necessário para ingerir os dados. O código aqui escrito deve dar uma boa base para implementação de uma pipeline paralelizada e eficiente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que tenhamos consumido a API e pegado os dados, vamos ver o que podemos fazer com eles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
